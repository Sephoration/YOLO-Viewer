"""
YOLOç»Ÿä¸€åˆ†æå™¨
æ•´åˆä¸‰ä¸ªæ¨¡å¼ï¼šæ£€æµ‹ã€åˆ†ç±»ã€å…³é”®ç‚¹
ç›´æ¥è°ƒç”¨æ¨¡å‹å¯¹è±¡ï¼Œç¦æ­¢ä½¿ç”¨.predict()æ–¹æ³•
å¤„ç†å¥½çš„æ•°æ®ç›´æ¥å åŠ åœ¨å±•ç¤ºçª—å£ä¸Š
æ•°æ®æ ¼å¼ç»Ÿä¸€åŒ–
åŠ å…¥äº†é¢„çƒ­åŠŸèƒ½
åŠ å…¥äº†ç±»åˆ«è¿‡æ»¤
"""

import cv2
import numpy as np
import torch
import time
from pathlib import Path
from typing import Union, Dict, Any, List, Optional, Tuple
from ultralytics import YOLO

from baseDetect import baseDetect

# YOLOç»Ÿä¸€åˆ†æå™¨å®ç° - å°†é¢„çƒ­åŠŸèƒ½é›†æˆåˆ°ä¸‹é¢çš„UnifiedYOLOç±»ä¸­


class UnifiedYOLO(baseDetect):
    """
    ç»Ÿä¸€YOLOå¤„ç†å™¨ - æ•´åˆä¸‰ä¸ªæ¨¡å¼
    éµå¾ªè€å¸ˆè¦æ±‚çš„ä»£ç é£æ ¼ï¼šç›´æ¥è°ƒç”¨æ¨¡å‹å¯¹è±¡
    """
    
    def __init__(self, model_path: str, mode: str = 'auto',
                 conf_threshold: float = 0.25, iou_threshold: float = 0.7,
                 warmup: bool = True, config_path: str = None):
        """
        åˆå§‹åŒ–ç»Ÿä¸€YOLOå¤„ç†å™¨
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            mode: æ¨¡å¼ ('auto', 'detection', 'classification', 'pose')
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            iou_threshold: IOUé˜ˆå€¼
            warmup: æ˜¯å¦åœ¨åŠ è½½æ¨¡å‹æ—¶æ‰§è¡Œé¢„çƒ­
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨ä¸æ¨¡å‹åŒåçš„.jsonæ–‡ä»¶
        """
        super().__init__()
        
        self.model_path = model_path
        self.mode = self._detect_mode(model_path) if mode == 'auto' else mode
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        self.warmup = warmup  # é¢„çƒ­å¼€å…³
        self.config_path = config_path  # é…ç½®æ–‡ä»¶è·¯å¾„
        
        # è®¾å¤‡é€‰æ‹©
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # æ¨¡å‹å¯¹è±¡ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self.model = None
        self.model_info = {}
        self.warmed_up = False  # é¢„çƒ­çŠ¶æ€æ ‡å¿—
        
        # å¯è§†åŒ–é…ç½®
        self._setup_visualization_params()
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        self._load_config()
        
        # æ¨¡å¼ç‰¹å®šå‚æ•°
        self._setup_mode_params()
        
        # å¯è§†åŒ–æ§åˆ¶å‚æ•°
        self.show_kpt_names = True  # æ˜¯å¦æ˜¾ç¤ºå…³é”®ç‚¹åç§°
        self.show_skeleton = True   # æ˜¯å¦æ˜¾ç¤ºéª¨æ¶è¿æ¥
        self.show_bbox = True       # æ˜¯å¦æ˜¾ç¤ºè¾¹ç•Œæ¡†
        
        print(f"YOLOç»Ÿä¸€å¤„ç†å™¨åˆå§‹åŒ– | æ¨¡å¼: {self.mode} | è®¾å¤‡: {self.device}")
    
    def _detect_mode(self, model_path: str) -> str:
        """
        è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            str: æ¨¡å‹æ¨¡å¼ ('detection', 'classification', 'pose', 'segmentation')
        """
        filename = Path(model_path).name.lower()
        
        # æ ¹æ®æ–‡ä»¶ååˆ¤æ–­
        if 'cls' in filename or 'classify' in filename:
            return 'classification'
        elif 'pose' in filename or 'keypoint' in filename:
            return 'pose'
        elif 'seg' in filename:
            return 'segmentation'
        elif 'det' in filename or 'obj' in filename:
            return 'detection'
        else:
            # é»˜è®¤æ£€æµ‹æ¨¡å¼
            return 'detection'
    
    def _setup_mode_params(self):
        """æ ¹æ®æ¨¡å¼è®¾ç½®å‚æ•°"""
        if self.mode == 'pose':
            self.conf = 0.3
            self.iou = 0.6
            self.img_size = 640
        elif self.mode == 'classification':
            self.conf = 0.25
            self.iou = 0.45
            self.img_size = 224  # åˆ†ç±»æ¨¡å‹é€šå¸¸ä½¿ç”¨224
        elif self.mode == 'segmentation':
            self.conf = 0.25
            self.iou = 0.7
            self.img_size = 640
        else:  # detection
            self.conf = 0.25
            self.iou = 0.7
            self.img_size = 640
        
        # è¦†ç›–ç”¨æˆ·ä¼ å…¥çš„å‚æ•°
        self.conf = self.conf_threshold if self.conf_threshold else self.conf
        self.iou = self.iou_threshold if self.iou_threshold else self.iou
    
    def _setup_visualization_params(self):
        """è®¾ç½®å¯è§†åŒ–é…ç½®å‚æ•°"""
        # åŸºç¡€æ¡†é…ç½®
        self.bbox_color = (150, 0, 0)            # æ¡†çš„ BGR é¢œè‰²
        self.bbox_thickness = 2                   # æ¡†çš„çº¿å®½
        self.bbox_labelstr = {
            'font_size': 0.5,         # å­—ä½“å¤§å°
            'font_thickness': 1,   # å­—ä½“ç²—ç»†
            'offset_x': 0,          # X æ–¹å‘ï¼Œæ–‡å­—åç§»è·ç¦»ï¼Œå‘å³ä¸ºæ­£
            'offset_y': -10,        # Y æ–¹å‘ï¼Œæ–‡å­—åç§»è·ç¦»ï¼Œå‘ä¸‹ä¸ºæ­£
        }
        
        # å…³é”®ç‚¹é»˜è®¤é…ç½® - æ”¯æŒäººä½“17å…³é”®ç‚¹å’Œè‡ªå®šä¹‰å…³é”®ç‚¹
        self.kpt_color_map = {
            # äººä½“17å…³é”®ç‚¹é»˜è®¤é…ç½®
            0: {'name': 'nose', 'color': [255, 0, 0], 'radius': 3},          # é¼»å­
            1: {'name': 'left_eye', 'color': [0, 255, 0], 'radius': 3},      # å·¦çœ¼
            2: {'name': 'right_eye', 'color': [0, 0, 255], 'radius': 3},     # å³çœ¼
            3: {'name': 'left_ear', 'color': [255, 255, 0], 'radius': 3},    # å·¦è€³
            4: {'name': 'right_ear', 'color': [255, 0, 255], 'radius': 3},   # å³è€³
            5: {'name': 'left_shoulder', 'color': [0, 255, 255], 'radius': 4}, # å·¦è‚©
            6: {'name': 'right_shoulder', 'color': [128, 0, 0], 'radius': 4}, # å³è‚©
            7: {'name': 'left_elbow', 'color': [0, 128, 0], 'radius': 4},    # å·¦è‚˜
            8: {'name': 'right_elbow', 'color': [0, 0, 128], 'radius': 4},   # å³è‚˜
            9: {'name': 'left_wrist', 'color': [128, 128, 0], 'radius': 4},  # å·¦æ‰‹è…•
            10: {'name': 'right_wrist', 'color': [128, 0, 128], 'radius': 4}, # å³æ‰‹è…•
            11: {'name': 'left_hip', 'color': [0, 128, 128], 'radius': 4},   # å·¦é«‹
            12: {'name': 'right_hip', 'color': [64, 0, 0], 'radius': 4},     # å³é«‹
            13: {'name': 'left_knee', 'color': [0, 64, 0], 'radius': 4},     # å·¦è†
            14: {'name': 'right_knee', 'color': [0, 0, 64], 'radius': 4},    # å³è†
            15: {'name': 'left_ankle', 'color': [64, 64, 0], 'radius': 4},   # å·¦è„šè¸
            16: {'name': 'right_ankle', 'color': [64, 0, 64], 'radius': 4},  # å³è„šè¸
        }
        
        # å…³é”®ç‚¹ç±»åˆ«æ–‡å­—é…ç½®
        self.kpt_labelstr = {
            'font_size': 0.4,             # å­—ä½“å¤§å°
            'font_thickness': 1,       # å­—ä½“ç²—ç»†
            'offset_x': 5,             # X æ–¹å‘ï¼Œæ–‡å­—åç§»è·ç¦»ï¼Œå‘å³ä¸ºæ­£
            'offset_y': 5,            # Y æ–¹å‘ï¼Œæ–‡å­—åç§»è·ç¦»ï¼Œå‘ä¸‹ä¸ºæ­£
        }
        
        # éª¨æ¶è¿æ¥ BGR é…è‰²æ–¹æ¡ˆ
        self.skeleton_map = [
            # äººä½“17å…³é”®ç‚¹éª¨æ¶è¿æ¥
            {'srt_kpt_id': 0, 'dst_kpt_id': 1, 'color': [196, 75, 255], 'thickness': 2},  # é¼»å­-å·¦çœ¼
            {'srt_kpt_id': 0, 'dst_kpt_id': 2, 'color': [196, 75, 255], 'thickness': 2},  # é¼»å­-å³çœ¼
            {'srt_kpt_id': 1, 'dst_kpt_id': 3, 'color': [196, 75, 255], 'thickness': 2},  # å·¦çœ¼-å·¦è€³
            {'srt_kpt_id': 2, 'dst_kpt_id': 4, 'color': [196, 75, 255], 'thickness': 2},  # å³çœ¼-å³è€³
            {'srt_kpt_id': 0, 'dst_kpt_id': 5, 'color': [196, 75, 255], 'thickness': 2},  # é¼»å­-å·¦è‚©
            {'srt_kpt_id': 0, 'dst_kpt_id': 6, 'color': [196, 75, 255], 'thickness': 2},  # é¼»å­-å³è‚©
            {'srt_kpt_id': 5, 'dst_kpt_id': 6, 'color': [196, 75, 255], 'thickness': 2},  # å·¦è‚©-å³è‚©
            {'srt_kpt_id': 5, 'dst_kpt_id': 7, 'color': [196, 75, 255], 'thickness': 2},  # å·¦è‚©-å·¦è‚˜
            {'srt_kpt_id': 6, 'dst_kpt_id': 8, 'color': [196, 75, 255], 'thickness': 2},  # å³è‚©-å³è‚˜
            {'srt_kpt_id': 7, 'dst_kpt_id': 9, 'color': [196, 75, 255], 'thickness': 2},  # å·¦è‚˜-å·¦æ‰‹è…•
            {'srt_kpt_id': 8, 'dst_kpt_id': 10, 'color': [196, 75, 255], 'thickness': 2},  # å³è‚˜-å³æ‰‹è…•
            {'srt_kpt_id': 5, 'dst_kpt_id': 11, 'color': [196, 75, 255], 'thickness': 2},  # å·¦è‚©-å·¦é«‹
            {'srt_kpt_id': 6, 'dst_kpt_id': 12, 'color': [196, 75, 255], 'thickness': 2},  # å³è‚©-å³é«‹
            {'srt_kpt_id': 11, 'dst_kpt_id': 12, 'color': [196, 75, 255], 'thickness': 2},  # å·¦é«‹-å³é«‹
            {'srt_kpt_id': 11, 'dst_kpt_id': 13, 'color': [196, 75, 255], 'thickness': 2},  # å·¦é«‹-å·¦è†
            {'srt_kpt_id': 12, 'dst_kpt_id': 14, 'color': [196, 75, 255], 'thickness': 2},  # å³é«‹-å³è†
            {'srt_kpt_id': 13, 'dst_kpt_id': 15, 'color': [196, 75, 255], 'thickness': 2},  # å·¦è†-å·¦è„šè¸
            {'srt_kpt_id': 14, 'dst_kpt_id': 16, 'color': [196, 75, 255], 'thickness': 2},  # å³è†-å³è„šè¸
        ]
        
        # å¯è§†åŒ–æ§åˆ¶å‚æ•°
        self.show_kpt_names = True  # æ˜¯å¦æ˜¾ç¤ºå…³é”®ç‚¹åç§°
        self.show_skeleton = True   # æ˜¯å¦æ˜¾ç¤ºéª¨æ¶è¿æ¥
        self.show_bbox = True       # æ˜¯å¦æ˜¾ç¤ºè¾¹ç•Œæ¡†
    
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        import json
        import os
        
        # å¦‚æœæ²¡æœ‰æä¾›é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå°è¯•ä½¿ç”¨ä¸æ¨¡å‹åŒåçš„.jsonæ–‡ä»¶
        if not self.config_path:
            # è·å–æ¨¡å‹æ–‡ä»¶çš„ç›®å½•å’Œæ–‡ä»¶åï¼ˆä¸å¸¦æ‰©å±•åï¼‰
            model_dir = os.path.dirname(self.model_path)
            model_name = os.path.splitext(os.path.basename(self.model_path))[0]
            self.config_path = os.path.join(model_dir, f"{model_name}.json")
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.config_path):
            print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {self.config_path}")
            return
        
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # æ›´æ–°é…ç½®
            self._update_config(config)
            print(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
        except Exception as e:
            print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    def _update_config(self, config):
        """æ›´æ–°é…ç½®"""
        # æ›´æ–°è¾¹ç•Œæ¡†é…ç½®
        if 'bbox' in config:
            bbox_config = config['bbox']
            if 'color' in bbox_config:
                self.bbox_color = tuple(bbox_config['color'])
            if 'thickness' in bbox_config:
                self.bbox_thickness = bbox_config['thickness']
            if 'label' in bbox_config:
                self.bbox_labelstr.update(bbox_config['label'])
        
        # æ›´æ–°å…³é”®ç‚¹é…ç½®
        if 'keypoints' in config:
            kpt_config = config['keypoints']
            # è½¬æ¢å­—ç¬¦ä¸²é”®ä¸ºæ•´æ•°
            kpt_config = {int(k): v for k, v in kpt_config.items()}
            self.kpt_color_map.update(kpt_config)
        
        # æ›´æ–°å…³é”®ç‚¹æ ‡ç­¾é…ç½®
        if 'keypoint_label' in config:
            self.kpt_labelstr.update(config['keypoint_label'])
        
        # æ›´æ–°éª¨æ¶è¿æ¥é…ç½®
        if 'skeleton' in config:
            self.skeleton_map = config['skeleton']
        
        # æ›´æ–°å¯è§†åŒ–æ§åˆ¶å‚æ•°
        if 'visualization' in config:
            vis_config = config['visualization']
            if 'show_keypoint_names' in vis_config:
                self.show_kpt_names = vis_config['show_keypoint_names']
            if 'show_skeleton' in vis_config:
                self.show_skeleton = vis_config['show_skeleton']
            if 'show_bbox' in vis_config:
                self.show_bbox = vis_config['show_bbox']
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self.model is not None:
            return True
        
        try:
            # ç¡®ä¿ä½¿ç”¨ç›¸å¯¹è·¯å¾„æ˜¾ç¤º
            model_name = Path(self.model_path).name
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
            
            # âœ… è€å¸ˆçš„æ–¹å¼ï¼šç›´æ¥åˆ›å»ºYOLOå¯¹è±¡ï¼Œä¸ä½¿ç”¨.predict()
            self.model = YOLO(self.model_path)
            self.model.to(self.device)
            
            # æ”¶é›†æ¨¡å‹ä¿¡æ¯
            self._collect_model_info()
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            
            # æ‰§è¡Œé¢„çƒ­ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.warmup and not self.warmed_up:
                self._perform_warmup()
                
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _collect_model_info(self):
        """æ”¶é›†æ¨¡å‹ä¿¡æ¯"""
        if self.model is None:
            return
        
        # åˆå§‹åŒ–åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…å«é»˜è®¤å€¼
        self.model_info = {
            'mode': self.mode,
            'device': self.device,
            'input_size': self.img_size,
            'conf_threshold': self.conf,
            'iou_threshold': self.iou,
            'class_names': [],  # é»˜è®¤ç©ºåˆ—è¡¨
            'class_count': 'æœªçŸ¥',  # é»˜è®¤'æœªçŸ¥'
            'task': 'æœªçŸ¥'  # é»˜è®¤'æœªçŸ¥'
        }
        
        # å°è¯•è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯å¹¶è¦†ç›–é»˜è®¤å€¼
        try:
            if hasattr(self.model, 'names'):
                # ç¡®ä¿values()è¿”å›çš„æ˜¯åˆ—è¡¨
                self.model_info['class_names'] = list(self.model.names.values())
                self.model_info['class_count'] = len(self.model.names)
            
            if hasattr(self.model, 'task'):
                self.model_info['task'] = self.model.task
        except Exception as e:  # æ•è·å…·ä½“å¼‚å¸¸
            print(f"[WARNING] æ”¶é›†æ¨¡å‹è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            # å³ä½¿å‡ºé”™ï¼Œä¹Ÿä¿ç•™é»˜è®¤å€¼
            pass
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if not self.model_info:
            self._collect_model_info()
        
        return self.model_info.copy()
    
    @staticmethod
    def analyze_model_info(model_path: str) -> Dict[str, Any]:
        """
        åˆ†ææ¨¡å‹ä¿¡æ¯ï¼ˆè½»é‡çº§ï¼Œä¸çœŸæ­£åŠ è½½æ¨¡å‹ï¼‰
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: æ¨¡å‹ä¿¡æ¯
        """
        try:
            import os
            from pathlib import Path
            
            filename = Path(model_path).name.lower()
            file_size = os.path.getsize(model_path)
            
            # æ ¹æ®æ–‡ä»¶åçŒœæµ‹æ¨¡å¼
            if 'cls' in filename or 'classify' in filename:
                task_type = 'classification'
                input_size = '224x224'
            elif 'pose' in filename or 'keypoint' in filename:
                task_type = 'pose'
                input_size = '640x640'
            elif 'seg' in filename:
                task_type = 'segmentation'
                input_size = '640x640'
            else:
                task_type = 'detection'
                input_size = '640x640'
            
            # å°è¯•åŠ è½½æ¨¡å‹è·å–æ›´å‡†ç¡®çš„ä¿¡æ¯
            try:
                model = YOLO(model_path)
                if hasattr(model, 'names'):
                    class_count = len(model.names)
                else:
                    class_count = 'æœªçŸ¥'
                    
                if hasattr(model, 'task'):
                    task_type = model.task
                    
                # é‡Šæ”¾æ¨¡å‹
                del model
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
            except:
                class_count = 'æœªçŸ¥'
            
            return {
                'model_name': Path(model_path).name,
                'task_type': task_type,
                'input_size': input_size,
                'class_count': class_count,
                'file_size': f"{file_size/1024/1024:.1f} MB"
            }
            
        except Exception as e:
            print(f"æ¨¡å‹ä¿¡æ¯åˆ†æå¤±è´¥: {e}")
            return None
    
    def process_frame(self, frame: np.ndarray) -> Dict[str, Any]:
        """
        å¤„ç†å•å¸§å›¾åƒ - ç»Ÿä¸€æ¥å£
        
        Args:
            frame: è¾“å…¥å›¾åƒ (BGRæ ¼å¼)
            
        Returns:
            Dict: å¤„ç†ç»“æœï¼ŒåŒ…å«å›¾åƒå’Œç»Ÿè®¡ä¿¡æ¯
        """
        start_time = time.time()
        
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        if not self.load_model():
            return {
                'success': False,
                'error': 'æ¨¡å‹åŠ è½½å¤±è´¥',
                'image': frame,
                'stats': {}
            }
        
        try:
            # æ ¹æ®æ¨¡å¼è°ƒç”¨ä¸åŒçš„å¤„ç†æ–¹æ³•
            if self.mode == 'classification':
                result_dict = self._process_classification(frame)
            elif self.mode == 'pose':
                result_dict = self._process_pose(frame)
            elif self.mode == 'segmentation':
                result_dict = self._process_segmentation(frame)
            else:  # detection
                result_dict = self._process_detection(frame)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            inference_time = time.time() - start_time
            
            # æ·»åŠ æ—¶é—´ä¿¡æ¯åˆ°ç»Ÿè®¡
            if 'stats' in result_dict:
                result_dict['stats']['inference_time'] = inference_time * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                result_dict['stats']['fps'] = 1.0 / inference_time if inference_time > 0 else 0
            
            result_dict['success'] = True
            return result_dict
            
        except Exception as e:
            print(f"å¸§å¤„ç†å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'image': frame,
                'stats': {
                    'detection_count': 0,
                    'avg_confidence': 0.0,
                    'inference_time': 0,
                    'fps': 0.0
                }
            }
    
    def _process_detection(self, frame: np.ndarray) -> Dict[str, Any]:
        """
        å¤„ç†ç›®æ ‡æ£€æµ‹
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            
        Returns:
            Dict: æ£€æµ‹ç»“æœ
        """
        # âœ… è€å¸ˆçš„æ–¹å¼ï¼šç›´æ¥è°ƒç”¨æ¨¡å‹å¯¹è±¡
        results = self.model(
            frame,
            conf=self.conf,
            iou=self.iou,
            imgsz=self.img_size,
            verbose=False
        )
        
        result = results[0]
        
        # æå–æ£€æµ‹ç»“æœ
        if result.boxes is None:
            return {
                'image': frame,
                'stats': {
                    'detection_count': 0,
                    'avg_confidence': 0.0
                }
            }
        
        # æå–è¾¹ç•Œæ¡†
        boxes = result.boxes.xyxy.cpu().numpy() if result.boxes.xyxy is not None else []
        confidences = result.boxes.conf.cpu().numpy() if result.boxes.conf is not None else []
        class_ids = result.boxes.cls.cpu().numpy().astype(int) if result.boxes.cls is not None else []
        
        # æå–ç±»åˆ«åç§°
        class_names = []
        for cls_id in class_ids:
            if hasattr(result, 'names') and cls_id < len(result.names):
                class_names.append(result.names[cls_id])
            else:
                class_names.append(f"object_{cls_id}")
        
        # æ„å»ºæ£€æµ‹ç»“æœåˆ—è¡¨ï¼ˆç”¨äºç”»æ¡†ï¼‰
        pred_boxes = []
        for i in range(len(boxes)):
            x1, y1, x2, y2 = boxes[i]
            lbl = class_names[i] if i < len(class_names) else f"object_{class_ids[i]}"
            confidence = confidences[i] if i < len(confidences) else 0.0
            track_id = None  # æ£€æµ‹æ¨¡å¼æ²¡æœ‰track_id
            
            pred_boxes.append((x1, y1, x2, y2, lbl, confidence, track_id))
        
        # ä½¿ç”¨åŸºç±»çš„ç”»æ¡†æ–¹æ³•
        processed_frame = self.draw_bboxes(frame, pred_boxes)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        detection_count = len(boxes)
        avg_confidence = np.mean(confidences) if len(confidences) > 0 else 0.0
        
        # ç±»åˆ«åˆ†å¸ƒ
        class_distribution = {}
        for cls_name in class_names:
            class_distribution[cls_name] = class_distribution.get(cls_name, 0) + 1
        
        return {
            'image': processed_frame,
            'stats': {
                'detection_count': detection_count,
                'avg_confidence': float(avg_confidence),
                'class_distribution': class_distribution,
                'mode': 'detection'
            },
            'raw_data': {
                'boxes': boxes,
                'confidences': confidences,
                'class_ids': class_ids,
                'class_names': class_names
            }
        }
    
    def _process_classification(self, frame: np.ndarray) -> Dict[str, Any]:
        """
        å¤„ç†å›¾åƒåˆ†ç±»
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            
        Returns:
            Dict: åˆ†ç±»ç»“æœ
        """
        # âœ… è€å¸ˆçš„æ–¹å¼ï¼šç›´æ¥è°ƒç”¨æ¨¡å‹å¯¹è±¡
        results = self.model(
            frame,
            conf=self.conf,
            imgsz=self.img_size,
            verbose=False
        )
        
        result = results[0]
        
        # æå–åˆ†ç±»ç»“æœ
        if hasattr(result, 'probs') and result.probs is not None:
            # è·å–æ¦‚ç‡å’Œç±»åˆ«
            probs = result.probs.data.cpu().numpy()
            top_idx = np.argsort(probs)[-1]  # æœ€é«˜æ¦‚ç‡çš„ç´¢å¼•
            top_prob = probs[top_idx]
            
            # è·å–ç±»åˆ«åç§°
            if hasattr(result, 'names'):
                top_class = result.names[top_idx]
            else:
                top_class = f"class_{top_idx}"
            
            # åœ¨å›¾åƒä¸Šç»˜åˆ¶åˆ†ç±»ç»“æœ
            processed_frame = frame.copy()
            
            # ä½¿ç”¨PILç»˜åˆ¶ä¸­æ–‡ï¼ˆä»è€å¸ˆä»£ç ä¸­å€Ÿé‰´ï¼‰
            from PIL import Image, ImageDraw, ImageFont
            import os
            
            # è½¬æ¢BGRåˆ°RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img_pil = Image.fromarray(frame_rgb)
            draw = ImageDraw.Draw(img_pil)
            
            # åŠ è½½å­—ä½“ - ä½¿ç”¨ç›¸å¯¹è·¯å¾„
            font_path = "SimHei.ttf"
            if os.path.exists(font_path):
                font = ImageFont.truetype(font_path, 20)
            else:
                print(f"è­¦å‘Š: æœªæ‰¾åˆ°å­—ä½“æ–‡ä»¶ {font_path}ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“")
                font = ImageFont.load_default()
            
            # ç»˜åˆ¶æ–‡æœ¬
            text = f"{top_class}: {top_prob:.2%}"
            text_position = (30, 30)
            
            # ç»˜åˆ¶è¾¹æ¡†ï¼ˆä»è€å¸ˆä»£ç ä¸­å€Ÿé‰´ï¼‰
            border_color = (255, 255, 255)
            border_width = 2
            for dx, dy in [(-border_width, 0), (border_width, 0), (0, -border_width), (0, border_width),
                          (-border_width, -border_width), (-border_width, border_width),
                          (border_width, -border_width), (border_width, border_width)]:
                draw.text((text_position[0] + dx, text_position[1] + dy), text, font=font, fill=border_color)
            
            # ç»˜åˆ¶æ­£æ–‡
            draw.text(text_position, text, font=font, fill=(255, 0, 0, 1))
            
            # è½¬æ¢å›BGR
            processed_frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
            
            return {
                'image': processed_frame,
                'stats': {
                    'detection_count': 1,  # åˆ†ç±»ä»»åŠ¡å›ºå®šä¸º1
                    'avg_confidence': float(top_prob),
                    'class_name': top_class,
                    'mode': 'classification'
                },
                'raw_data': {
                    'top_class': top_class,
                    'top_confidence': float(top_prob),
                    'all_probs': probs.tolist()
                }
            }
        else:
            # æ²¡æœ‰åˆ†ç±»ç»“æœ
            return {
                'image': frame,
                'stats': {
                    'detection_count': 0,
                    'avg_confidence': 0.0,
                    'mode': 'classification'
                }
            }
            
        print('[CLS]ç»˜åˆ¶å®Œæˆ', processed_frame.shape, processed_frame.dtype)
    
    def _process_pose(self, frame: np.ndarray) -> Dict[str, Any]:
        """
        å¤„ç†å…³é”®ç‚¹æ£€æµ‹
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            
        Returns:
            Dict: å§¿æ€ä¼°è®¡ç»“æœ
        """
        # âœ… è€å¸ˆçš„æ–¹å¼ï¼šç›´æ¥è°ƒç”¨æ¨¡å‹å¯¹è±¡
        results = self.model(
            frame,
            conf=self.conf,
            iou=self.iou,
            imgsz=self.img_size,
            verbose=False
        )
        
        result = results[0]
        
        # æå–å…³é”®ç‚¹ç»“æœ
        if result.boxes is None or result.keypoints is None:
            return {
                'image': frame,
                'stats': {
                    'detection_count': 0,
                    'avg_confidence': 0.0,
                    'mode': 'pose'
                }
            }
        
        # æå–è¾¹ç•Œæ¡†å’Œå…³é”®ç‚¹
        boxes = result.boxes.xyxy.cpu().numpy() if result.boxes.xyxy is not None else []
        confidences = result.boxes.conf.cpu().numpy() if result.boxes.conf is not None else []
        keypoints = result.keypoints.xy.cpu().numpy() if result.keypoints.xy is not None else []
        keypoints_conf = result.keypoints.conf.cpu().numpy() if result.keypoints.conf is not None else []
        
        # å¯è§†åŒ–å…³é”®ç‚¹
        processed_frame = frame.copy()
        
        for person_idx in range(len(boxes)):
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            if self.show_bbox and person_idx < len(boxes):
                box = boxes[person_idx]
                x1, y1, x2, y2 = map(int, box[:4])
                cv2.rectangle(processed_frame, (x1, y1), (x2, y2), self.bbox_color, self.bbox_thickness)
            
            # ç»˜åˆ¶å…³é”®ç‚¹
            if person_idx < len(keypoints):
                person_keypoints = keypoints[person_idx]
                
                # ç»˜åˆ¶éª¨æ¶è¿æ¥
                if self.show_skeleton:
                    for skeleton in self.skeleton_map:
                        start_idx = skeleton['srt_kpt_id']
                        end_idx = skeleton['dst_kpt_id']
                        if (start_idx < len(person_keypoints) and end_idx < len(person_keypoints)):
                            start_kp = person_keypoints[start_idx]
                            end_kp = person_keypoints[end_idx]
                            
                            # æ£€æŸ¥å…³é”®ç‚¹ç½®ä¿¡åº¦
                            start_conf = keypoints_conf[person_idx][start_idx] if (person_idx < len(keypoints_conf) and start_idx < len(keypoints_conf[person_idx])) else 1.0
                            end_conf = keypoints_conf[person_idx][end_idx] if (person_idx < len(keypoints_conf) and end_idx < len(keypoints_conf[person_idx])) else 1.0
                            
                            if start_conf > 0.1 and end_conf > 0.1:
                                color = skeleton['color']
                                thickness = skeleton['thickness']
                                cv2.line(processed_frame, 
                                        (int(start_kp[0]), int(start_kp[1])),
                                        (int(end_kp[0]), int(end_kp[1])),
                                        color, thickness)
                
                # ç»˜åˆ¶å…³é”®ç‚¹
                for kp_idx, kp in enumerate(person_keypoints):
                    kp_conf = keypoints_conf[person_idx][kp_idx] if (person_idx < len(keypoints_conf) and kp_idx < len(keypoints_conf[person_idx])) else 1.0
                    if kp_conf > 0.1:
                        # è·å–å…³é”®ç‚¹é…ç½®
                        kpt_config = self.kpt_color_map.get(kp_idx, {
                            'color': [0, 255, 0],
                            'radius': 3
                        })
                        
                        color = tuple(kpt_config['color'])
                        radius = kpt_config['radius']
                        
                        # ç»˜åˆ¶å…³é”®ç‚¹åœ†
                        cv2.circle(processed_frame, (int(kp[0]), int(kp[1])), radius, color, -1)
                        
                        # æ˜¾ç¤ºå…³é”®ç‚¹åç§°
                        if self.show_kpt_names and 'name' in kpt_config:
                            kpt_name = kpt_config['name']
                            font = cv2.FONT_HERSHEY_SIMPLEX
                            font_size = self.kpt_labelstr['font_size']
                            font_thickness = self.kpt_labelstr['font_thickness']
                            offset_x = self.kpt_labelstr['offset_x']
                            offset_y = self.kpt_labelstr['offset_y']
                            
                            text_x = int(kp[0]) + offset_x
                            text_y = int(kp[1]) + offset_y
                            
                            # ç»˜åˆ¶æ–‡æœ¬èƒŒæ™¯
                            (text_width, text_height), baseline = cv2.getTextSize(kpt_name, font, font_size, font_thickness)
                            bg_x1 = text_x
                            bg_y1 = text_y - text_height - baseline
                            bg_x2 = text_x + text_width
                            bg_y2 = text_y + baseline
                            cv2.rectangle(processed_frame, (bg_x1, bg_y1), (bg_x2, bg_y2), color, -1)
                            
                            # ç»˜åˆ¶æ–‡æœ¬
                            cv2.putText(processed_frame, kpt_name, (text_x, text_y), font, font_size, (0, 0, 0), font_thickness, cv2.LINE_AA)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        detection_count = len(boxes)
        avg_confidence = np.mean(confidences) if len(confidences) > 0 else 0.0
        
        # è®¡ç®—å…³é”®ç‚¹æ•°é‡
        total_keypoints = 0
        for i in range(len(keypoints)):
            if i < len(keypoints_conf):
                visible_keypoints = np.sum(keypoints_conf[i] > 0.1)
                total_keypoints += visible_keypoints
        
        return {
            'image': processed_frame,
            'stats': {
                'detection_count': detection_count,
                'avg_confidence': float(avg_confidence),
                'keypoint_count': total_keypoints,
                'mode': 'pose'
            },
            'raw_data': {
                'boxes': boxes,
                'confidences': confidences,
                'keypoints': keypoints,
                'keypoints_conf': keypoints_conf
            }
        }
    
    def _process_segmentation(self, frame: np.ndarray) -> Dict[str, Any]:
        """
        å¤„ç†åˆ†å‰²æ£€æµ‹
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            
        Returns:
            Dict: åˆ†å‰²ç»“æœ
        """
        # âœ… è€å¸ˆçš„æ–¹å¼ï¼šç›´æ¥è°ƒç”¨æ¨¡å‹å¯¹è±¡
        results = self.model(
            frame,
            conf=self.conf,
            iou=self.iou,
            imgsz=self.img_size,
            verbose=False
        )
        
        result = results[0]
        
        # æå–åˆ†å‰²ç»“æœ
        if result.masks is None:
            return {
                'image': frame,
                'stats': {
                    'detection_count': 0,
                    'avg_confidence': 0.0,
                    'mode': 'segmentation'
                }
            }
        
        # è·å–åˆ†å‰²æ©ç 
        masks = result.masks.data.cpu().numpy() if result.masks.data is not None else []
        boxes = result.boxes.xyxy.cpu().numpy() if result.boxes.xyxy is not None else []
        confidences = result.boxes.conf.cpu().numpy() if result.boxes.conf is not None else []
        class_ids = result.boxes.cls.cpu().numpy().astype(int) if result.boxes.cls is not None else []
        
        # å¯è§†åŒ–åˆ†å‰²ç»“æœ
        processed_frame = frame.copy()
        
        # ä¸ºæ¯ä¸ªæ©ç åˆ†é…é¢œè‰²
        colors = [
            (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
            (255, 0, 255), (0, 255, 255), (128, 0, 128)
        ]
        
        for i in range(len(masks)):
            mask = masks[i]
            class_id = class_ids[i] if i < len(class_ids) else 0
            color = colors[class_id % len(colors)]
            
            # å°†æ©ç è½¬æ¢ä¸ºäºŒå€¼å›¾åƒ
            mask_binary = (mask > 0).astype(np.uint8) * 255
            
            # åˆ›å»ºå½©è‰²æ©ç 
            mask_colored = np.zeros_like(frame)
            mask_colored[:, :, 0] = color[0] * (mask_binary / 255.0)
            mask_colored[:, :, 1] = color[1] * (mask_binary / 255.0)
            mask_colored[:, :, 2] = color[2] * (mask_binary / 255.0)
            
            # å åŠ æ©ç åˆ°åŸå›¾ï¼ˆåŠé€æ˜ï¼‰
            alpha = 0.3
            processed_frame = cv2.addWeighted(processed_frame, 1, mask_colored.astype(np.uint8), alpha, 0)
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            if i < len(boxes):
                box = boxes[i]
                x1, y1, x2, y2 = map(int, box[:4])
                cv2.rectangle(processed_frame, (x1, y1), (x2, y2), color, 2)
                
                # æ·»åŠ æ ‡ç­¾
                if hasattr(result, 'names') and class_id < len(result.names):
                    label = result.names[class_id]
                else:
                    label = f"class_{class_id}"
                
                conf = confidences[i] if i < len(confidences) else 0.0
                label_text = f"{label} {conf:.2f}"
                
                # è®¡ç®—æ–‡æœ¬å¤§å°
                (text_width, text_height), baseline = cv2.getTextSize(
                    label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2
                )
                
                # ç»˜åˆ¶æ–‡æœ¬èƒŒæ™¯
                cv2.rectangle(processed_frame, (x1, y1 - text_height - 10), 
                             (x1 + text_width, y1), color, -1)
                
                # ç»˜åˆ¶æ–‡æœ¬
                cv2.putText(processed_frame, label_text, (x1, y1 - 5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        detection_count = len(masks)
        avg_confidence = np.mean(confidences) if len(confidences) > 0 else 0.0
        
        # ç±»åˆ«åˆ†å¸ƒ
        class_distribution = {}
        for cls_id in class_ids:
            if hasattr(result, 'names') and cls_id < len(result.names):
                cls_name = result.names[cls_id]
            else:
                cls_name = f"class_{cls_id}"
            class_distribution[cls_name] = class_distribution.get(cls_name, 0) + 1
        
        return {
            'image': processed_frame,
            'stats': {
                'detection_count': detection_count,
                'avg_confidence': float(avg_confidence),
                'class_distribution': class_distribution,
                'mode': 'segmentation'
            },
            'raw_data': {
                'masks': masks,
                'boxes': boxes,
                'confidences': confidences,
                'class_ids': class_ids
            }
        }
    
    def _perform_warmup(self):
        """
        æ‰§è¡Œæ¨¡å‹é¢„çƒ­ï¼Œé’ˆå¯¹ä¸åŒæ¨¡å‹ç±»å‹ä½¿ç”¨åˆé€‚çš„è¾“å…¥å°ºå¯¸
        é¢„çƒ­å¯ä»¥å‡å°‘é¦–æ¬¡æ¨ç†çš„å»¶è¿Ÿï¼Œç‰¹åˆ«æ˜¯å¯¹äºGPUæ¨¡å‹
        """
        if self.model is None:
            return
            
        print(f"ğŸ”„ å¼€å§‹æ¨¡å‹é¢„çƒ­ | æ¨¡å¼: {self.mode} | è¾“å…¥å°ºå¯¸: {self.img_size}x{self.img_size}")
        start_time = time.time()
        
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨åˆé€‚çš„è¾“å…¥å°ºå¯¸
            # åˆ†ç±»æ¨¡å‹é€šå¸¸ä½¿ç”¨224ï¼Œå…¶ä»–æ¨¡å‹ä½¿ç”¨640
            warmup_size = self.img_size  # å·²ç»æ ¹æ®æ¨¡å¼è®¾ç½®å¥½äº†æ­£ç¡®çš„å°ºå¯¸
            
            # åˆ›å»ºè™šæ‹Ÿè¾“å…¥æ•°æ®
            dummy_input = np.random.randint(0, 255, (warmup_size, warmup_size, 3), dtype=np.uint8)
            
            # ä½¿ç”¨torch.no_grad()å‡å°‘å†…å­˜ä½¿ç”¨
            with torch.no_grad():
                # è¿›è¡Œå¤šæ¬¡é¢„çƒ­æ¨ç†ï¼ˆé€šå¸¸3-5æ¬¡è¶³å¤Ÿï¼‰
                for i in range(3):
                    # æ‰§è¡Œæ¨ç†ï¼Œä½†ä¸å¤„ç†ç»“æœ
                    _ = self.model(
                        dummy_input, 
                        conf=self.conf, 
                        iou=self.iou, 
                        imgsz=warmup_size, 
                        verbose=False
                    )
            
            # æ¸…ç†ç¼“å­˜ï¼ˆç‰¹åˆ«æ˜¯åœ¨GPUä¸Šè¿è¡Œæ—¶ï¼‰
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            self.warmed_up = True
            warmup_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            print(f"âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ | è€—æ—¶: {warmup_time:.2f}ms")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}")
            # é¢„çƒ­å¤±è´¥ä¸å½±å“æ¨¡å‹ä½¿ç”¨ï¼Œåªæ˜¯é¦–æ¬¡æ¨ç†å¯èƒ½è¾ƒæ…¢
    
    def update_params(self, conf_threshold=None, iou_threshold=None):
        """å®æ—¶æ›´æ–°æ¨ç†å‚æ•°
        
        Args:
            conf_threshold: æ–°çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰
            iou_threshold: æ–°çš„IOUé˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰
        
        Returns:
            bool: å‚æ•°æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            if conf_threshold is not None:
                # éªŒè¯ç½®ä¿¡åº¦é˜ˆå€¼èŒƒå›´
                if 0.0 <= conf_threshold <= 1.0:
                    self.conf = conf_threshold
                    self.conf_threshold = conf_threshold  # æ›´æ–°åŸå§‹å±æ€§ä»¥ä¾¿ä¿æŒä¸€è‡´æ€§
                    print(f"[INFO] ç½®ä¿¡åº¦é˜ˆå€¼æ›´æ–°ä¸º: {conf_threshold}")
                else:
                    print(f"[ERROR] ç½®ä¿¡åº¦é˜ˆå€¼å¿…é¡»åœ¨0.0åˆ°1.0ä¹‹é—´ï¼Œæä¾›çš„å€¼: {conf_threshold}")
                    return False
            
            if iou_threshold is not None:
                # éªŒè¯IOUé˜ˆå€¼èŒƒå›´
                if 0.0 <= iou_threshold <= 1.0:
                    self.iou = iou_threshold
                    self.iou_threshold = iou_threshold  # æ›´æ–°åŸå§‹å±æ€§ä»¥ä¾¿ä¿æŒä¸€è‡´æ€§
                    print(f"[INFO] IOUé˜ˆå€¼æ›´æ–°ä¸º: {iou_threshold}")
                else:
                    print(f"[ERROR] IOUé˜ˆå€¼å¿…é¡»åœ¨0.0åˆ°1.0ä¹‹é—´ï¼Œæä¾›çš„å€¼: {iou_threshold}")
                    return False
            
            return True
        except Exception as e:
            print(f"[ERROR] æ›´æ–°å‚æ•°æ—¶å‡ºé”™: {e}")
            return False
    
    def __call__(self, frame: np.ndarray) -> Dict[str, Any]:
        """ä½¿å¯¹è±¡å¯è°ƒç”¨"""
        return self.process_frame(frame)