"""
YOLOç»Ÿä¸€åˆ†æå™¨
æ•´åˆä¸‰ä¸ªæ¨¡å¼ï¼šæ£€æµ‹ã€åˆ†ç±»ã€å…³é”®ç‚¹
ç›´æ¥è°ƒç”¨æ¨¡å‹å¯¹è±¡ï¼Œç¦æ­¢ä½¿ç”¨.predict()æ–¹æ³•
å¤„ç†å¥½çš„æ•°æ®ç›´æ¥å åŠ åœ¨å±•ç¤ºçª—å£ä¸Š
æ•°æ®æ ¼å¼ç»Ÿä¸€åŒ–
åŠ å…¥äº†é¢„çƒ­åŠŸèƒ½
"""

import cv2
import numpy as np
import torch
import time
from pathlib import Path
from typing import Union, Dict, Any, List, Optional, Tuple
from ultralytics import YOLO

from baseDetect import baseDetect

# YOLOç»Ÿä¸€åˆ†æå™¨å®ç° - å°†é¢„çƒ­åŠŸèƒ½é›†æˆåˆ°ä¸‹é¢çš„UnifiedYOLOç±»ä¸­


class UnifiedYOLO(baseDetect):
    """
    ç»Ÿä¸€YOLOå¤„ç†å™¨ - æ•´åˆä¸‰ä¸ªæ¨¡å¼
    éµå¾ªè€å¸ˆè¦æ±‚çš„ä»£ç é£æ ¼ï¼šç›´æ¥è°ƒç”¨æ¨¡å‹å¯¹è±¡
    """
    
    def __init__(self, model_path: str, mode: str = 'auto',
                 conf_threshold: float = 0.25, iou_threshold: float = 0.7,
                 warmup: bool = True):
        """
        åˆå§‹åŒ–ç»Ÿä¸€YOLOå¤„ç†å™¨
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            mode: æ¨¡å¼ ('auto', 'detection', 'classification', 'pose')
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            iou_threshold: IOUé˜ˆå€¼
            warmup: æ˜¯å¦åœ¨åŠ è½½æ¨¡å‹æ—¶æ‰§è¡Œé¢„çƒ­
        """
        super().__init__()
        
        self.model_path = model_path
        self.mode = self._detect_mode(model_path) if mode == 'auto' else mode
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        self.warmup = warmup  # é¢„çƒ­å¼€å…³
        
        # è®¾å¤‡é€‰æ‹©
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # æ¨¡å‹å¯¹è±¡ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self.model = None
        self.model_info = {}
        self.warmed_up = False  # é¢„çƒ­çŠ¶æ€æ ‡å¿—
        
        # æ¨¡å¼ç‰¹å®šå‚æ•°
        self._setup_mode_params()
        
        print(f"YOLOç»Ÿä¸€å¤„ç†å™¨åˆå§‹åŒ– | æ¨¡å¼: {self.mode} | è®¾å¤‡: {self.device}")
    
    def _detect_mode(self, model_path: str) -> str:
        """
        è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            str: æ¨¡å‹æ¨¡å¼ ('detection', 'classification', 'pose', 'segmentation')
        """
        filename = Path(model_path).name.lower()
        
        # æ ¹æ®æ–‡ä»¶ååˆ¤æ–­
        if 'cls' in filename or 'classify' in filename:
            return 'classification'
        elif 'pose' in filename or 'keypoint' in filename:
            return 'pose'
        elif 'seg' in filename:
            return 'segmentation'
        elif 'det' in filename or 'obj' in filename:
            return 'detection'
        else:
            # é»˜è®¤æ£€æµ‹æ¨¡å¼
            return 'detection'
    
    def _setup_mode_params(self):
        """æ ¹æ®æ¨¡å¼è®¾ç½®å‚æ•°"""
        if self.mode == 'pose':
            self.conf = 0.3
            self.iou = 0.6
            self.img_size = 640
        elif self.mode == 'classification':
            self.conf = 0.25
            self.iou = 0.45
            self.img_size = 224  # åˆ†ç±»æ¨¡å‹é€šå¸¸ä½¿ç”¨224
        elif self.mode == 'segmentation':
            self.conf = 0.25
            self.iou = 0.7
            self.img_size = 640
        else:  # detection
            self.conf = 0.25
            self.iou = 0.7
            self.img_size = 640
        
        # è¦†ç›–ç”¨æˆ·ä¼ å…¥çš„å‚æ•°
        self.conf = self.conf_threshold if self.conf_threshold else self.conf
        self.iou = self.iou_threshold if self.iou_threshold else self.iou
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self.model is not None:
            return True
        
        try:
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")
            
            # âœ… è€å¸ˆçš„æ–¹å¼ï¼šç›´æ¥åˆ›å»ºYOLOå¯¹è±¡ï¼Œä¸ä½¿ç”¨.predict()
            self.model = YOLO(self.model_path)
            self.model.to(self.device)
            
            # æ”¶é›†æ¨¡å‹ä¿¡æ¯
            self._collect_model_info()
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {Path(self.model_path).name}")
            
            # æ‰§è¡Œé¢„çƒ­ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.warmup and not self.warmed_up:
                self._perform_warmup()
                
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _collect_model_info(self):
        """æ”¶é›†æ¨¡å‹ä¿¡æ¯"""
        if self.model is None:
            return
        
        # åˆå§‹åŒ–åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…å«é»˜è®¤å€¼
        self.model_info = {
            'mode': self.mode,
            'device': self.device,
            'input_size': self.img_size,
            'conf_threshold': self.conf,
            'iou_threshold': self.iou,
            'class_names': [],  # é»˜è®¤ç©ºåˆ—è¡¨
            'class_count': 'æœªçŸ¥',  # é»˜è®¤'æœªçŸ¥'
            'task': 'æœªçŸ¥'  # é»˜è®¤'æœªçŸ¥'
        }
        
        # å°è¯•è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯å¹¶è¦†ç›–é»˜è®¤å€¼
        try:
            if hasattr(self.model, 'names'):
                # ç¡®ä¿values()è¿”å›çš„æ˜¯åˆ—è¡¨
                self.model_info['class_names'] = list(self.model.names.values())
                self.model_info['class_count'] = len(self.model.names)
            
            if hasattr(self.model, 'task'):
                self.model_info['task'] = self.model.task
        except Exception as e:  # æ•è·å…·ä½“å¼‚å¸¸
            print(f"[WARNING] æ”¶é›†æ¨¡å‹è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            # å³ä½¿å‡ºé”™ï¼Œä¹Ÿä¿ç•™é»˜è®¤å€¼
            pass
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if not self.model_info:
            self._collect_model_info()
        
        return self.model_info.copy()
    
    @staticmethod
    def analyze_model_info(model_path: str) -> Dict[str, Any]:
        """
        åˆ†ææ¨¡å‹ä¿¡æ¯ï¼ˆè½»é‡çº§ï¼Œä¸çœŸæ­£åŠ è½½æ¨¡å‹ï¼‰
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: æ¨¡å‹ä¿¡æ¯
        """
        try:
            import os
            from pathlib import Path
            
            filename = Path(model_path).name.lower()
            file_size = os.path.getsize(model_path)
            
            # æ ¹æ®æ–‡ä»¶åçŒœæµ‹æ¨¡å¼
            if 'cls' in filename or 'classify' in filename:
                task_type = 'classification'
                input_size = '224x224'
            elif 'pose' in filename or 'keypoint' in filename:
                task_type = 'pose'
                input_size = '640x640'
            elif 'seg' in filename:
                task_type = 'segmentation'
                input_size = '640x640'
            else:
                task_type = 'detection'
                input_size = '640x640'
            
            # å°è¯•åŠ è½½æ¨¡å‹è·å–æ›´å‡†ç¡®çš„ä¿¡æ¯
            try:
                model = YOLO(model_path)
                if hasattr(model, 'names'):
                    class_count = len(model.names)
                else:
                    class_count = 'æœªçŸ¥'
                    
                if hasattr(model, 'task'):
                    task_type = model.task
                    
                # é‡Šæ”¾æ¨¡å‹
                del model
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
            except:
                class_count = 'æœªçŸ¥'
            
            return {
                'model_name': Path(model_path).name,
                'task_type': task_type,
                'input_size': input_size,
                'class_count': class_count,
                'file_size': f"{file_size/1024/1024:.1f} MB"
            }
            
        except Exception as e:
            print(f"æ¨¡å‹ä¿¡æ¯åˆ†æå¤±è´¥: {e}")
            return None
    
    def process_frame(self, frame: np.ndarray) -> Dict[str, Any]:
        """
        å¤„ç†å•å¸§å›¾åƒ - ç»Ÿä¸€æ¥å£
        
        Args:
            frame: è¾“å…¥å›¾åƒ (BGRæ ¼å¼)
            
        Returns:
            Dict: å¤„ç†ç»“æœï¼ŒåŒ…å«å›¾åƒå’Œç»Ÿè®¡ä¿¡æ¯
        """
        start_time = time.time()
        
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        if not self.load_model():
            return {
                'success': False,
                'error': 'æ¨¡å‹åŠ è½½å¤±è´¥',
                'image': frame,
                'stats': {}
            }
        
        try:
            # æ ¹æ®æ¨¡å¼è°ƒç”¨ä¸åŒçš„å¤„ç†æ–¹æ³•
            if self.mode == 'classification':
                result_dict = self._process_classification(frame)
            elif self.mode == 'pose':
                result_dict = self._process_pose(frame)
            elif self.mode == 'segmentation':
                result_dict = self._process_segmentation(frame)
            else:  # detection
                result_dict = self._process_detection(frame)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            inference_time = time.time() - start_time
            
            # æ·»åŠ æ—¶é—´ä¿¡æ¯åˆ°ç»Ÿè®¡
            if 'stats' in result_dict:
                result_dict['stats']['inference_time'] = inference_time * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                result_dict['stats']['fps'] = 1.0 / inference_time if inference_time > 0 else 0
            
            result_dict['success'] = True
            return result_dict
            
        except Exception as e:
            print(f"å¸§å¤„ç†å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'image': frame,
                'stats': {
                    'detection_count': 0,
                    'avg_confidence': 0.0,
                    'inference_time': 0,
                    'fps': 0.0
                }
            }
    
    def _process_detection(self, frame: np.ndarray) -> Dict[str, Any]:
        """
        å¤„ç†ç›®æ ‡æ£€æµ‹
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            
        Returns:
            Dict: æ£€æµ‹ç»“æœ
        """
        # âœ… è€å¸ˆçš„æ–¹å¼ï¼šç›´æ¥è°ƒç”¨æ¨¡å‹å¯¹è±¡
        results = self.model(
            frame,
            conf=self.conf,
            iou=self.iou,
            imgsz=self.img_size,
            verbose=False
        )
        
        result = results[0]
        
        # æå–æ£€æµ‹ç»“æœ
        if result.boxes is None:
            return {
                'image': frame,
                'stats': {
                    'detection_count': 0,
                    'avg_confidence': 0.0
                }
            }
        
        # æå–è¾¹ç•Œæ¡†
        boxes = result.boxes.xyxy.cpu().numpy() if result.boxes.xyxy is not None else []
        confidences = result.boxes.conf.cpu().numpy() if result.boxes.conf is not None else []
        class_ids = result.boxes.cls.cpu().numpy().astype(int) if result.boxes.cls is not None else []
        
        # æå–ç±»åˆ«åç§°
        class_names = []
        for cls_id in class_ids:
            if hasattr(result, 'names') and cls_id < len(result.names):
                class_names.append(result.names[cls_id])
            else:
                class_names.append(f"object_{cls_id}")
        
        # æ„å»ºæ£€æµ‹ç»“æœåˆ—è¡¨ï¼ˆç”¨äºç”»æ¡†ï¼‰
        pred_boxes = []
        for i in range(len(boxes)):
            x1, y1, x2, y2 = boxes[i]
            lbl = class_names[i] if i < len(class_names) else f"object_{class_ids[i]}"
            confidence = confidences[i] if i < len(confidences) else 0.0
            track_id = None  # æ£€æµ‹æ¨¡å¼æ²¡æœ‰track_id
            
            pred_boxes.append((x1, y1, x2, y2, lbl, confidence, track_id))
        
        # ä½¿ç”¨åŸºç±»çš„ç”»æ¡†æ–¹æ³•
        processed_frame = self.draw_bboxes(frame, pred_boxes)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        detection_count = len(boxes)
        avg_confidence = np.mean(confidences) if len(confidences) > 0 else 0.0
        
        # ç±»åˆ«åˆ†å¸ƒ
        class_distribution = {}
        for cls_name in class_names:
            class_distribution[cls_name] = class_distribution.get(cls_name, 0) + 1
        
        return {
            'image': processed_frame,
            'stats': {
                'detection_count': detection_count,
                'avg_confidence': float(avg_confidence),
                'class_distribution': class_distribution,
                'mode': 'detection'
            },
            'raw_data': {
                'boxes': boxes,
                'confidences': confidences,
                'class_ids': class_ids,
                'class_names': class_names
            }
        }
    
    def _process_classification(self, frame: np.ndarray) -> Dict[str, Any]:
        """
        å¤„ç†å›¾åƒåˆ†ç±»
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            
        Returns:
            Dict: åˆ†ç±»ç»“æœ
        """
        # âœ… è€å¸ˆçš„æ–¹å¼ï¼šç›´æ¥è°ƒç”¨æ¨¡å‹å¯¹è±¡
        results = self.model(
            frame,
            conf=self.conf,
            imgsz=self.img_size,
            verbose=False
        )
        
        result = results[0]
        
        # æå–åˆ†ç±»ç»“æœ
        if hasattr(result, 'probs') and result.probs is not None:
            # è·å–æ¦‚ç‡å’Œç±»åˆ«
            probs = result.probs.data.cpu().numpy()
            top_idx = np.argsort(probs)[-1]  # æœ€é«˜æ¦‚ç‡çš„ç´¢å¼•
            top_prob = probs[top_idx]
            
            # è·å–ç±»åˆ«åç§°
            if hasattr(result, 'names'):
                top_class = result.names[top_idx]
            else:
                top_class = f"class_{top_idx}"
            
            # åœ¨å›¾åƒä¸Šç»˜åˆ¶åˆ†ç±»ç»“æœ
            processed_frame = frame.copy()
            
            # ä½¿ç”¨PILç»˜åˆ¶ä¸­æ–‡ï¼ˆä»è€å¸ˆä»£ç ä¸­å€Ÿé‰´ï¼‰
            from PIL import Image, ImageDraw, ImageFont
            import os
            
            # è½¬æ¢BGRåˆ°RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img_pil = Image.fromarray(frame_rgb)
            draw = ImageDraw.Draw(img_pil)
            
            # åŠ è½½å­—ä½“
            font_path = "SimHei.ttf"
            if os.path.exists(font_path):
                font = ImageFont.truetype(font_path, 20)
            else:
                font = ImageFont.load_default()
            
            # ç»˜åˆ¶æ–‡æœ¬
            text = f"{top_class}: {top_prob:.2%}"
            text_position = (30, 30)
            
            # ç»˜åˆ¶è¾¹æ¡†ï¼ˆä»è€å¸ˆä»£ç ä¸­å€Ÿé‰´ï¼‰
            border_color = (255, 255, 255)
            border_width = 2
            for dx, dy in [(-border_width, 0), (border_width, 0), (0, -border_width), (0, border_width),
                          (-border_width, -border_width), (-border_width, border_width),
                          (border_width, -border_width), (border_width, border_width)]:
                draw.text((text_position[0] + dx, text_position[1] + dy), text, font=font, fill=border_color)
            
            # ç»˜åˆ¶æ­£æ–‡
            draw.text(text_position, text, font=font, fill=(255, 0, 0, 1))
            
            # è½¬æ¢å›BGR
            processed_frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)
            
            return {
                'image': processed_frame,
                'stats': {
                    'detection_count': 1,  # åˆ†ç±»ä»»åŠ¡å›ºå®šä¸º1
                    'avg_confidence': float(top_prob),
                    'class_name': top_class,
                    'mode': 'classification'
                },
                'raw_data': {
                    'top_class': top_class,
                    'top_confidence': float(top_prob),
                    'all_probs': probs.tolist()
                }
            }
        else:
            # æ²¡æœ‰åˆ†ç±»ç»“æœ
            return {
                'image': frame,
                'stats': {
                    'detection_count': 0,
                    'avg_confidence': 0.0,
                    'mode': 'classification'
                }
            }
            
        print('[CLS]ç»˜åˆ¶å®Œæˆ', processed_frame.shape, processed_frame.dtype)
    
    def _process_pose(self, frame: np.ndarray) -> Dict[str, Any]:
        """
        å¤„ç†å…³é”®ç‚¹æ£€æµ‹
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            
        Returns:
            Dict: å§¿æ€ä¼°è®¡ç»“æœ
        """
        # âœ… è€å¸ˆçš„æ–¹å¼ï¼šç›´æ¥è°ƒç”¨æ¨¡å‹å¯¹è±¡
        results = self.model(
            frame,
            conf=self.conf,
            iou=self.iou,
            imgsz=self.img_size,
            verbose=False
        )
        
        result = results[0]
        
        # æå–å…³é”®ç‚¹ç»“æœ
        if result.boxes is None or result.keypoints is None:
            return {
                'image': frame,
                'stats': {
                    'detection_count': 0,
                    'avg_confidence': 0.0,
                    'mode': 'pose'
                }
            }
        
        # æå–è¾¹ç•Œæ¡†å’Œå…³é”®ç‚¹
        boxes = result.boxes.xyxy.cpu().numpy() if result.boxes.xyxy is not None else []
        confidences = result.boxes.conf.cpu().numpy() if result.boxes.conf is not None else []
        keypoints = result.keypoints.xy.cpu().numpy() if result.keypoints.xy is not None else []
        keypoints_conf = result.keypoints.conf.cpu().numpy() if result.keypoints.conf is not None else []
        
        # å¯è§†åŒ–å…³é”®ç‚¹
        processed_frame = frame.copy()
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†å’Œå…³é”®ç‚¹
        skeleton_connections = [
            (0, 1), (0, 2), (1, 3), (2, 4),  # è„¸éƒ¨
            (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # èº¯å¹²
            (5, 11), (6, 12), (11, 13), (13, 15), (12, 14), (14, 16)  # å››è‚¢
        ]
        
        skeleton_colors = [
            (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
            (255, 0, 255), (0, 255, 255), (128, 0, 0), (0, 128, 0)
        ]
        
        for person_idx in range(len(boxes)):
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            if person_idx < len(boxes):
                box = boxes[person_idx]
                x1, y1, x2, y2 = map(int, box[:4])
                cv2.rectangle(processed_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            
            # ç»˜åˆ¶å…³é”®ç‚¹
            if person_idx < len(keypoints):
                person_keypoints = keypoints[person_idx]
                
                # ç»˜åˆ¶éª¨æ¶è¿æ¥
                for connection in skeleton_connections:
                    start_idx, end_idx = connection
                    if (start_idx < len(person_keypoints) and end_idx < len(person_keypoints)):
                        start_kp = person_keypoints[start_idx]
                        end_kp = person_keypoints[end_idx]
                        
                        # æ£€æŸ¥å…³é”®ç‚¹ç½®ä¿¡åº¦
                        start_conf = keypoints_conf[person_idx][start_idx] if (person_idx < len(keypoints_conf) and start_idx < len(keypoints_conf[person_idx])) else 1.0
                        end_conf = keypoints_conf[person_idx][end_idx] if (person_idx < len(keypoints_conf) and end_idx < len(keypoints_conf[person_idx])) else 1.0
                        
                        if start_conf > 0.1 and end_conf > 0.1:
                            color = skeleton_colors[connection[0] % len(skeleton_colors)]
                            cv2.line(processed_frame, 
                                    (int(start_kp[0]), int(start_kp[1])),
                                    (int(end_kp[0]), int(end_kp[1])),
                                    color, 2)
                
                # ç»˜åˆ¶å…³é”®ç‚¹
                for kp_idx, kp in enumerate(person_keypoints):
                    kp_conf = keypoints_conf[person_idx][kp_idx] if (person_idx < len(keypoints_conf) and kp_idx < len(keypoints_conf[person_idx])) else 1.0
                    if kp_conf > 0.1:
                        color_intensity = int(255 * kp_conf)
                        color = (0, color_intensity, 255 - color_intensity)
                        cv2.circle(processed_frame, (int(kp[0]), int(kp[1])), 3, color, -1)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        detection_count = len(boxes)
        avg_confidence = np.mean(confidences) if len(confidences) > 0 else 0.0
        
        # è®¡ç®—å…³é”®ç‚¹æ•°é‡
        total_keypoints = 0
        for i in range(len(keypoints)):
            if i < len(keypoints_conf):
                visible_keypoints = np.sum(keypoints_conf[i] > 0.1)
                total_keypoints += visible_keypoints
        
        return {
            'image': processed_frame,
            'stats': {
                'detection_count': detection_count,
                'avg_confidence': float(avg_confidence),
                'keypoint_count': total_keypoints,
                'mode': 'pose'
            },
            'raw_data': {
                'boxes': boxes,
                'confidences': confidences,
                'keypoints': keypoints,
                'keypoints_conf': keypoints_conf
            }
        }
    
    def _process_segmentation(self, frame: np.ndarray) -> Dict[str, Any]:
        """
        å¤„ç†åˆ†å‰²æ£€æµ‹
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            
        Returns:
            Dict: åˆ†å‰²ç»“æœ
        """
        # âœ… è€å¸ˆçš„æ–¹å¼ï¼šç›´æ¥è°ƒç”¨æ¨¡å‹å¯¹è±¡
        results = self.model(
            frame,
            conf=self.conf,
            iou=self.iou,
            imgsz=self.img_size,
            verbose=False
        )
        
        result = results[0]
        
        # æå–åˆ†å‰²ç»“æœ
        if result.masks is None:
            return {
                'image': frame,
                'stats': {
                    'detection_count': 0,
                    'avg_confidence': 0.0,
                    'mode': 'segmentation'
                }
            }
        
        # è·å–åˆ†å‰²æ©ç 
        masks = result.masks.data.cpu().numpy() if result.masks.data is not None else []
        boxes = result.boxes.xyxy.cpu().numpy() if result.boxes.xyxy is not None else []
        confidences = result.boxes.conf.cpu().numpy() if result.boxes.conf is not None else []
        class_ids = result.boxes.cls.cpu().numpy().astype(int) if result.boxes.cls is not None else []
        
        # å¯è§†åŒ–åˆ†å‰²ç»“æœ
        processed_frame = frame.copy()
        
        # ä¸ºæ¯ä¸ªæ©ç åˆ†é…é¢œè‰²
        colors = [
            (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),
            (255, 0, 255), (0, 255, 255), (128, 0, 128)
        ]
        
        for i in range(len(masks)):
            mask = masks[i]
            class_id = class_ids[i] if i < len(class_ids) else 0
            color = colors[class_id % len(colors)]
            
            # å°†æ©ç è½¬æ¢ä¸ºäºŒå€¼å›¾åƒ
            mask_binary = (mask > 0).astype(np.uint8) * 255
            
            # åˆ›å»ºå½©è‰²æ©ç 
            mask_colored = np.zeros_like(frame)
            mask_colored[:, :, 0] = color[0] * (mask_binary / 255.0)
            mask_colored[:, :, 1] = color[1] * (mask_binary / 255.0)
            mask_colored[:, :, 2] = color[2] * (mask_binary / 255.0)
            
            # å åŠ æ©ç åˆ°åŸå›¾ï¼ˆåŠé€æ˜ï¼‰
            alpha = 0.3
            processed_frame = cv2.addWeighted(processed_frame, 1, mask_colored.astype(np.uint8), alpha, 0)
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            if i < len(boxes):
                box = boxes[i]
                x1, y1, x2, y2 = map(int, box[:4])
                cv2.rectangle(processed_frame, (x1, y1), (x2, y2), color, 2)
                
                # æ·»åŠ æ ‡ç­¾
                if hasattr(result, 'names') and class_id < len(result.names):
                    label = result.names[class_id]
                else:
                    label = f"class_{class_id}"
                
                conf = confidences[i] if i < len(confidences) else 0.0
                label_text = f"{label} {conf:.2f}"
                
                # è®¡ç®—æ–‡æœ¬å¤§å°
                (text_width, text_height), baseline = cv2.getTextSize(
                    label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2
                )
                
                # ç»˜åˆ¶æ–‡æœ¬èƒŒæ™¯
                cv2.rectangle(processed_frame, (x1, y1 - text_height - 10), 
                             (x1 + text_width, y1), color, -1)
                
                # ç»˜åˆ¶æ–‡æœ¬
                cv2.putText(processed_frame, label_text, (x1, y1 - 5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        detection_count = len(masks)
        avg_confidence = np.mean(confidences) if len(confidences) > 0 else 0.0
        
        # ç±»åˆ«åˆ†å¸ƒ
        class_distribution = {}
        for cls_id in class_ids:
            if hasattr(result, 'names') and cls_id < len(result.names):
                cls_name = result.names[cls_id]
            else:
                cls_name = f"class_{cls_id}"
            class_distribution[cls_name] = class_distribution.get(cls_name, 0) + 1
        
        return {
            'image': processed_frame,
            'stats': {
                'detection_count': detection_count,
                'avg_confidence': float(avg_confidence),
                'class_distribution': class_distribution,
                'mode': 'segmentation'
            },
            'raw_data': {
                'masks': masks,
                'boxes': boxes,
                'confidences': confidences,
                'class_ids': class_ids
            }
        }
    
    def _perform_warmup(self):
        """
        æ‰§è¡Œæ¨¡å‹é¢„çƒ­ï¼Œé’ˆå¯¹ä¸åŒæ¨¡å‹ç±»å‹ä½¿ç”¨åˆé€‚çš„è¾“å…¥å°ºå¯¸
        é¢„çƒ­å¯ä»¥å‡å°‘é¦–æ¬¡æ¨ç†çš„å»¶è¿Ÿï¼Œç‰¹åˆ«æ˜¯å¯¹äºGPUæ¨¡å‹
        """
        if self.model is None:
            return
            
        print(f"ğŸ”„ å¼€å§‹æ¨¡å‹é¢„çƒ­ | æ¨¡å¼: {self.mode} | è¾“å…¥å°ºå¯¸: {self.img_size}x{self.img_size}")
        start_time = time.time()
        
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨åˆé€‚çš„è¾“å…¥å°ºå¯¸
            # åˆ†ç±»æ¨¡å‹é€šå¸¸ä½¿ç”¨224ï¼Œå…¶ä»–æ¨¡å‹ä½¿ç”¨640
            warmup_size = self.img_size  # å·²ç»æ ¹æ®æ¨¡å¼è®¾ç½®å¥½äº†æ­£ç¡®çš„å°ºå¯¸
            
            # åˆ›å»ºè™šæ‹Ÿè¾“å…¥æ•°æ®
            dummy_input = np.random.randint(0, 255, (warmup_size, warmup_size, 3), dtype=np.uint8)
            
            # ä½¿ç”¨torch.no_grad()å‡å°‘å†…å­˜ä½¿ç”¨
            with torch.no_grad():
                # è¿›è¡Œå¤šæ¬¡é¢„çƒ­æ¨ç†ï¼ˆé€šå¸¸3-5æ¬¡è¶³å¤Ÿï¼‰
                for i in range(3):
                    # æ‰§è¡Œæ¨ç†ï¼Œä½†ä¸å¤„ç†ç»“æœ
                    _ = self.model(
                        dummy_input, 
                        conf=self.conf, 
                        iou=self.iou, 
                        imgsz=warmup_size, 
                        verbose=False
                    )
            
            # æ¸…ç†ç¼“å­˜ï¼ˆç‰¹åˆ«æ˜¯åœ¨GPUä¸Šè¿è¡Œæ—¶ï¼‰
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            self.warmed_up = True
            warmup_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            print(f"âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ | è€—æ—¶: {warmup_time:.2f}ms")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}")
            # é¢„çƒ­å¤±è´¥ä¸å½±å“æ¨¡å‹ä½¿ç”¨ï¼Œåªæ˜¯é¦–æ¬¡æ¨ç†å¯èƒ½è¾ƒæ…¢
    
    def update_params(self, conf_threshold=None, iou_threshold=None):
        """å®æ—¶æ›´æ–°æ¨ç†å‚æ•°
        
        Args:
            conf_threshold: æ–°çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰
            iou_threshold: æ–°çš„IOUé˜ˆå€¼ï¼ˆ0.0-1.0ï¼‰
        
        Returns:
            bool: å‚æ•°æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            if conf_threshold is not None:
                # éªŒè¯ç½®ä¿¡åº¦é˜ˆå€¼èŒƒå›´
                if 0.0 <= conf_threshold <= 1.0:
                    self.conf = conf_threshold
                    self.conf_threshold = conf_threshold  # æ›´æ–°åŸå§‹å±æ€§ä»¥ä¾¿ä¿æŒä¸€è‡´æ€§
                    print(f"[INFO] ç½®ä¿¡åº¦é˜ˆå€¼æ›´æ–°ä¸º: {conf_threshold}")
                else:
                    print(f"[ERROR] ç½®ä¿¡åº¦é˜ˆå€¼å¿…é¡»åœ¨0.0åˆ°1.0ä¹‹é—´ï¼Œæä¾›çš„å€¼: {conf_threshold}")
                    return False
            
            if iou_threshold is not None:
                # éªŒè¯IOUé˜ˆå€¼èŒƒå›´
                if 0.0 <= iou_threshold <= 1.0:
                    self.iou = iou_threshold
                    self.iou_threshold = iou_threshold  # æ›´æ–°åŸå§‹å±æ€§ä»¥ä¾¿ä¿æŒä¸€è‡´æ€§
                    print(f"[INFO] IOUé˜ˆå€¼æ›´æ–°ä¸º: {iou_threshold}")
                else:
                    print(f"[ERROR] IOUé˜ˆå€¼å¿…é¡»åœ¨0.0åˆ°1.0ä¹‹é—´ï¼Œæä¾›çš„å€¼: {iou_threshold}")
                    return False
            
            return True
        except Exception as e:
            print(f"[ERROR] æ›´æ–°å‚æ•°æ—¶å‡ºé”™: {e}")
            return False
    
    def __call__(self, frame: np.ndarray) -> Dict[str, Any]:
        """ä½¿å¯¹è±¡å¯è°ƒç”¨"""
        return self.process_frame(frame)